# AI Papers

**Start:** 2025-01-04
**End:** TODO

TODO: reorg as pre-training, post-training, diffusion, multimodality, reasoning, RL, systems, etc?

- [ ] ML Fundamentals: [[2025-01-04-ml-fundamental-papers.md]]
  - [ ] Variational Autoencoders: [[2025-08-08-variational-autoencoder-papers.md]]
- [ ] Foundational LLMs: [[2025-01-04-foundational-llm-papers.md]]
- [ ] Speech/Audio: [[2025-07-16-speech-papers.md]]
- [ ] Vision: [[2025-07-21-vision-papers.md]]
- [ ] Distributed Optimization: [[2025-06-07-distributed-optimization-papers.md]]

## Reinforcement Learning

- TODO
- [ ] DPO: <https://arxiv.org/abs/2305.18290>
- [ ] IMPALA
- [ ] torchbeast

## Diffusion

- [ ] [2015] Deep Unsupervised Learning using Nonequilibrium Thermodynamics. <https://arxiv.org/abs/1503.03585>
- [ ] [2020] Denoising Diffusion Probabilistic Models. <https://arxiv.org/abs/2006.11239>
- [ ] [2022] The Annotated Diffusion Model. <https://huggingface.co/blog/annotated-diffusion>
- [ ] TODO

## Reasoning

- [ ] TODO

## Systems

- [ ] [2025] The Ultra-Scale Playbook: Training LLMs on GPU Clusters, HuggingFace. <https://huggingface.co/spaces/nanotron/ultrascale-playbook>
- [ ] [2025] <https://jax-ml.github.io/scaling-book>
- [X] [2025] [Internal] Llama4 pre-training deep dive. <https://docs.google.com/presentation/d/1vQYbmAPwqu9yWR0M0GjMoBv7qvAx2rjPhfxG2VMzKng/edit?usp=sharing>

## Misc

- [ ] [2024] MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. <https://arxiv.org/abs/2402.14905>

## Notes

### [2025] The Ultra-Scale Playbook: Training LLMs on GPU Clusters, HuggingFace

**Date:** 2025-02-08
**Blog:** <https://huggingface.co/spaces/nanotron/ultrascale-playbook>

- Training on 1 GPU
  - Assumes model params, grad-buffer, optimizer states can fit in single GPU.
  - Peak memory usage due to activations memory.
  - Reduce peak activations memory via
    - (1) activation checkpointing/recomputation
    - (2) gradient accumulation. global_batch_size = micro_batch_size x grad_acc_steps
- Collective Ops
  - Broadcast
  - Reduce, AllReduce
  - Gather, AllGather
  - Scatter, ReduceScatter
  - Ring AllReduce: ReduceScatter (N-1 ring steps) followed by AllGather (N-1 ring steps)
- Data Parallelism (DP)
  - global_batch_size = micro_batch_size x grad_acc_steps x num_gpus
  - Standard DP: entire model params, gradients and optimizer states replicated on all GPUs
    - Forward -> Backward -> AllReduce(grads) -> Update
  - DeepSpeed ZeRO (Zero Redundancy Optimizer): further memory optimization by sharding redundant memory
    - (1) ZeRO-1: optimizer states sharded across DP ranks
      - Forward -> Backward -> ReduceScatter(grads) -> Update -> AllGather(params)
      - The final AllGather over params can be overlapped with the next forward pass.
    - (2) ZeRO-2: optimizer states and gradients sharded across DP ranks
      - Same as ZeRO-1, except that grads are also sharded right after ReduceScatter.
      - Kinda weird why anyone would use ZeRO-1. This is a natural optimization, just cleaning up unwated grads corresponding to local batch after ReduceScatter.
    - (3) ZeRO-3 / FSDP: optimzier states, gradients and params sharded across DP ranks
      - In addition to ZeRO-2, also shard params across DP ranks.
      - Also called FSDP (Fully Sharded Data Parallel).
      - Params of each layer is sharded over DP ranks and materialized on the fly as needed via AllGather.
      - Reduces memory over ZeRO-2 at the cost of added 1 AllGather per layer per forward and per backward. For $L$ layers, $2 \times L - 1$ additional AllGather ops per training step (it's not $2 \times L$ as the last layer needs only one AllGather).
      - Forward computation for layer $L$ overlapped with AllGather for layer $L + 1$. Backward computation for layer $L$ overlapped with AllGather for layer $L - 1$.
      - Aside: Toy implementation in <https://github.com/viswanathgs/LLM-Training-Puzzles> [[2024-12-16-srush-puzzles]].
- Tensor Parallelism (TP)
  - So far: activation checkpointing + gradient accumulation + FSDP. Limitations:
    - What if even 1 layer can't fit in a single GPU?
    - Params, grads and optimizer states are sharded with FSDP, but activation memory can still be very high for large sequence lengths.
  - Tensor Parallelism: Shard a tensor along a particular dimension.
    - Column-wise sharding (column-linear) and row-wise sharding (row-linear). Different communication primitives based on sharding type.
    - Example: $Y = X \times W$; $X$ = input tensor/activations, $W$ = weight matrix
    - Column-wise sharding (column-linear): $W_{col=i}$ - $W$ sharded among GPUs along column dim
      - 1. $X$ = Broadcast $X$ to all GPUs
      - 2. Parallel $Y_{col_i}$ = $X \times W_{col=i}$
      - 3. $Y$ = AllGather $Y_{col=i}$
    - Row-wise sharding (row-linear): $W_{row=i}$ - $W$ sharded among GPUs along row dim
      - 1. $X_{col=i}$ = Scatter $X$ to across GPUs along column dim
      - 2. Parallel $Y_{gpu=i}$ = $X_{col=i} \times W_{row=i}$
      - 3. $Y$ = AllReduce $Y_{gpu=i}$
  - Tensor Parallelism in a transformer:
    - Feedforward block: 2 linear layers
      - Column-linear (without the final allgather) followed by row-linear (without needing the first step to scatter along column dim)
      - Goal: $Y1 = X \times W1$, $Y2 = Y1 \times W2$
      - Steps: $W1_{col=i}$ - $W1$ sharded along column dim, $W2_{row=i}$ - $W2$ sharded along row dim.
        - 1. $X$ = Broadcast $X$ to all GPUs
        - 2. Parallel $Y1_{col=i}$ = $X \times W1_{col=i}$
        - 3. Parallel $Y2_{gpu=i}$ = $Y1_{col=i} \times W2_{row=i}$
        - 4. $Y2$ = AllReduce $Y2_{gpu=i}$
    - Multihead attention block:
      - TODO
    - TODO
- Context Parallelism (CP)
  - TODO
- Pipeline Parallelism (PP)
  - Aside: Toy implementation in <https://github.com/viswanathgs/LLM-Training-Puzzles> [[2024-12-16-srush-puzzles]].
  - Layers split among a few GPUs, sequential dataflow.
  - Pro: Compared to Tensor Parallelism (TP), communication only at certain layer junctions. TP needs communications for each layer.
  - Downside: Lots of GPU idle time due to "bubble" when other layers are executing.
  - Size of the bubble / idle time can be reduced by microbatching (gradient accumulation) and pipelining execution across GPUs.
  - Bubble size = (p - 1) / m. p = degree of pipeline parallelism, m = microbatch size.
  - Methods:
    - 1. AFAB (All Forward All Backward):
      - Simplest approach, forward for all microbatches followed by backward.
      - Bubble time reduced by a factor of microbatch size $m$. Still need to retain activations in memory for all microbatches until backward is done.
    - 2. 1F1B (1 Forward 1 Backward):
      - Alternative forward and backward to clear out activations as soon as possible.
      - Need to store activations only for $p$ samples (where $p$ is the degree of pipeline paralleism) instead of $m$ (where $m$ is the total number of samples in the microbatch).
    - 3. Interleaving Stages:
      - Reduces bubble size (as opposed to just activation memory requirement)
      - Used in llama 3.1
    - 4. ZeroBubble and DualPipe
      - Close to a "zero bubble" regime
      - Used in DeepSeek V3
      - TODO
- Expert Parallelism (EP)
  - TODO
- 5D Parallelism
  - TODO
- Finding the best training configuration
  - TODO

### [2023] Prompting Large Language Models with Speech Recognition Abilities

**Date:** 2025-03-04
**Arxiv:** <https://arxiv.org/abs/2307.11795>
**Paperpile:** <https://app.paperpile.com/view/?id=d24a1f36-56fd-47cf-b1d9-fbf2ee244e7b>

- Simple approach overall. The goal is to leverage llama (trained just on text, not multilingual) to perform ASR.
- Prepend text prompt with audio embeddings from a conformer trained with CTC.
  - The initial input would be a sequence of audio embeddings followed by the <BOS> token at which point the LLM would start emitting text (hopefully transcribing the audio) auto-regressively.
  - "The ASR-LLM problem can possibly be reinterpreted as a copying/translation task where the LLM needs to regurgitate the information in the audio sequence. If the audio encoder provides a sequence of embeddings aligned with the text embeddings the problem collapses to a repetition task which should not require the full capacity of an LLM."
- Evaluate both using the LLM as is as well as LoRA-finetuned variant.
  - LoRA adaptor is only applied for the attention projection weights (q/k/v/o); feedforward, embedding and final linear layer weights remain frozen.
  - LoRA parameters: $\alpha = 16$ and $r = 8$.
- Section 4: Cosine similarly between the audio embeddings and the text embeddings (lookup table in llama mapping tokens to vectors) shows monotonic similarity.
- TODO: How does R = 0 (no LoRA, using the original LLM as is) work at all? Table 3
- Section 4: "The speech recognition task can be interpreted as a regurgitation task -- the language model is tasked with cleaning and repeating (in the same order) information that is present in the audio encoder output sequence."
  - "cleaning" is key

### [2024] Faster Speech-LLaMA Inference with Multi-token Prediction

**Date:** TODO
**Arxiv:** <https://arxiv.org/abs/2409.08148>
**Paperpile:** <https://app.paperpile.com/view/?id=204ddc58-b4d9-4e7d-9e51-edf21bd5e29f>

- TODO

### [2022] The Annotated Diffusion Model

**Date:** 2025-03-05
**Blog:** <https://huggingface.co/blog/annotated-diffusion>

- Two processes
  - Forward diffusion process: sample an image from the true distribution and gradually add gausian noise for $T$ steps until it's eventually pure noise / isotropic gaussian.
  - Reverse denoising diffusion process: neural net trained to gradually denoise an image starting from pure noise to an eventual image in the distribution.
- Forward diffusion process: $q(x_t | x_{t - 1})$. $x_0$ is the actual image and $x_T$ is pure noise.
  - At each step $t$, sample from a conditional gaussian distrubution with mean $\sqrt{1 - \beta_t}x_{t-1}$ and variance $\beta_tI$.
  - This can be done by sampling $\epsilon$ noise from the standard gaussian (0 mean, unit variance) and setting $x_t = \sqrt{1 - \beta_t}x_{t - 1} + \beta_t\epsilon$.
  - $\beta_t$ values change aross time steps following a "variance schedule" (can be linear, quadratic, cosine, etc), kinda like learning rate schedule.
- Backward denoising diffusion process:
  - In the forward diffusion process, starting with an actual sample $x_0$, if we set the schedule appropriately, we end up with pure gaussian noise at $x_T$.
- TODO

### [2024] LoRA Learns Less and Forgets Less

**Date:** 2025-05-06
**Arxiv:** <https://arxiv.org/abs/2405.09673>
**Paperpile:** TODO
